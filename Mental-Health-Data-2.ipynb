{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNKRQZCMis8CeuTtoNk7YYv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geopan2000/Exploring-Mental-Health-Data/blob/main/Mental-Health-Data-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-qSSzXu6sNOP",
        "outputId": "cfd48ed9-cf10-4ab7-c1e0-0e37c8518c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Exploring-Mental-Health-Data'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 57 (delta 21), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (57/57), 5.51 MiB | 3.34 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "/content/Exploring-Mental-Health-Data/Exploring-Mental-Health-Data/Exploring-Mental-Health-Data\n"
          ]
        }
      ],
      "source": [
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/geopan2000/Exploring-Mental-Health-Data.git\n",
        "\n",
        "# Change directory to the cloned repository\n",
        "%cd Exploring-Mental-Health-Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "!pip install xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install scikit-optimize\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "xy-6SSxfCTjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2eacca-34d2-422b-e26f-435f5ca2af5d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = pd.read_csv('/content/Exploring-Mental-Health-Data/data/train.csv')\n",
        "test_set = pd.read_csv('/content/Exploring-Mental-Health-Data/data/test.csv')"
      ],
      "metadata": {
        "id": "yh9AABJ5CX0f"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.head()"
      ],
      "metadata": {
        "id": "UQzDBpoXCip3",
        "outputId": "0395c421-3347-4e11-bd5d-458a9d39e7da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id      Name  Gender   Age           City Working Professional or Student  \\\n",
              "0   0  Aaradhya  Female  49.0       Ludhiana            Working Professional   \n",
              "1   1     Vivan    Male  26.0       Varanasi            Working Professional   \n",
              "2   2    Yuvraj    Male  33.0  Visakhapatnam                         Student   \n",
              "3   3    Yuvraj    Male  22.0         Mumbai            Working Professional   \n",
              "4   4      Rhea  Female  30.0         Kanpur            Working Professional   \n",
              "\n",
              "         Profession  Academic Pressure  Work Pressure  CGPA  \\\n",
              "0              Chef                NaN            5.0   NaN   \n",
              "1           Teacher                NaN            4.0   NaN   \n",
              "2               NaN                5.0            NaN  8.97   \n",
              "3           Teacher                NaN            5.0   NaN   \n",
              "4  Business Analyst                NaN            1.0   NaN   \n",
              "\n",
              "   Study Satisfaction  Job Satisfaction     Sleep Duration Dietary Habits  \\\n",
              "0                 NaN               2.0  More than 8 hours        Healthy   \n",
              "1                 NaN               3.0  Less than 5 hours      Unhealthy   \n",
              "2                 2.0               NaN          5-6 hours        Healthy   \n",
              "3                 NaN               1.0  Less than 5 hours       Moderate   \n",
              "4                 NaN               1.0          5-6 hours      Unhealthy   \n",
              "\n",
              "    Degree Have you ever had suicidal thoughts ?  Work/Study Hours  \\\n",
              "0      BHM                                    No               1.0   \n",
              "1      LLB                                   Yes               7.0   \n",
              "2  B.Pharm                                   Yes               3.0   \n",
              "3      BBA                                   Yes              10.0   \n",
              "4      BBA                                   Yes               9.0   \n",
              "\n",
              "   Financial Stress Family History of Mental Illness  Depression  \n",
              "0               2.0                               No           0  \n",
              "1               3.0                               No           1  \n",
              "2               1.0                               No           1  \n",
              "3               1.0                              Yes           1  \n",
              "4               4.0                              Yes           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39397b50-c01f-40c3-80fa-6a1cea1e98bd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Name</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>City</th>\n",
              "      <th>Working Professional or Student</th>\n",
              "      <th>Profession</th>\n",
              "      <th>Academic Pressure</th>\n",
              "      <th>Work Pressure</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>Study Satisfaction</th>\n",
              "      <th>Job Satisfaction</th>\n",
              "      <th>Sleep Duration</th>\n",
              "      <th>Dietary Habits</th>\n",
              "      <th>Degree</th>\n",
              "      <th>Have you ever had suicidal thoughts ?</th>\n",
              "      <th>Work/Study Hours</th>\n",
              "      <th>Financial Stress</th>\n",
              "      <th>Family History of Mental Illness</th>\n",
              "      <th>Depression</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Aaradhya</td>\n",
              "      <td>Female</td>\n",
              "      <td>49.0</td>\n",
              "      <td>Ludhiana</td>\n",
              "      <td>Working Professional</td>\n",
              "      <td>Chef</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>More than 8 hours</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>BHM</td>\n",
              "      <td>No</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Vivan</td>\n",
              "      <td>Male</td>\n",
              "      <td>26.0</td>\n",
              "      <td>Varanasi</td>\n",
              "      <td>Working Professional</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Less than 5 hours</td>\n",
              "      <td>Unhealthy</td>\n",
              "      <td>LLB</td>\n",
              "      <td>Yes</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>No</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Yuvraj</td>\n",
              "      <td>Male</td>\n",
              "      <td>33.0</td>\n",
              "      <td>Visakhapatnam</td>\n",
              "      <td>Student</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.97</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5-6 hours</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>B.Pharm</td>\n",
              "      <td>Yes</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Yuvraj</td>\n",
              "      <td>Male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Working Professional</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Less than 5 hours</td>\n",
              "      <td>Moderate</td>\n",
              "      <td>BBA</td>\n",
              "      <td>Yes</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Rhea</td>\n",
              "      <td>Female</td>\n",
              "      <td>30.0</td>\n",
              "      <td>Kanpur</td>\n",
              "      <td>Working Professional</td>\n",
              "      <td>Business Analyst</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5-6 hours</td>\n",
              "      <td>Unhealthy</td>\n",
              "      <td>BBA</td>\n",
              "      <td>Yes</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39397b50-c01f-40c3-80fa-6a1cea1e98bd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-39397b50-c01f-40c3-80fa-6a1cea1e98bd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-39397b50-c01f-40c3-80fa-6a1cea1e98bd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6cf94bfb-cb7d-4f1a-b27c-a8f5b6fc5ca9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6cf94bfb-cb7d-4f1a-b27c-a8f5b6fc5ca9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6cf94bfb-cb7d-4f1a-b27c-a8f5b6fc5ca9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_set"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Inspection"
      ],
      "metadata": {
        "id": "g7oFrxdGWvhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train_print = train_set.copy()\n",
        "#train_print = train_print.drop(columns=['Depression'])\n",
        "\n",
        "#for column in train_print.columns:\n",
        "    #train_nan_ratio = train_print[column].isna().sum() / len(train_print)  # Proportion of NaN in train set\n",
        "    #test_nan_ratio = test_set[column].isna().sum() / len(test_set)  # Proportion of NaN in test set\n",
        "    #print(f\"{column}  Train Set: {train_nan_ratio:.2%} Test Set: {test_nan_ratio:.2%}\")"
      ],
      "metadata": {
        "id": "KhM0uFVhZF9l"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the value counts for train and test sets\n",
        "#train_counts = train_set['Profession'].value_counts()\n",
        "#test_counts = test_set['Profession'].value_counts()\n",
        "# Get a combined set of unique values from both train and test sets\n",
        "#all_professions = set(train_counts.index).union(set(test_counts.index))\n",
        "\n",
        "# Print the counts for each profession\n",
        "#for profession in all_professions:\n",
        " #   train_count = train_counts.get(profession, 0)  # Get count from train, default to 0\n",
        "  #  test_count = test_counts.get(profession, 0)   # Get count from test, default to 0\n",
        "   # print(f\"{profession}: Train_set={train_count} Test_set={test_count}\")"
      ],
      "metadata": {
        "id": "uNd2rFkFcDAa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering:\n"
      ],
      "metadata": {
        "id": "qm9h56dJLbEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_id = train_set['id']\n",
        "\n",
        "target_variable = train_set['Depression']\n",
        "train_set.drop(columns=['id','Name','Depression'], inplace=True)\n",
        "test_id = test_set['id']\n",
        "\n",
        "test_set.drop(columns=['id','Name'], inplace=True)\n",
        "\n",
        "train_set['Gender'] = train_set['Gender'].map({'Male' : 1, 'Female' : 0}) # Always trasnforming 2 option object types into binary\n",
        "test_set['Gender'] = test_set['Gender'].map({'Male' : 1, 'Female' : 0})\n",
        "\n",
        "train_set['Age'] = train_set['Age'] / 100 # Normalized by the Max value\n",
        "train_set['Age'] = train_set['Age'].fillna(train_set['Age'].mean())\n",
        "test_set['Age'] = test_set['Age'] / 100\n",
        "test_set['Age'] = test_set['Age'].fillna(test_set['Age'].mean())\n",
        "\n",
        "train_set['Working Professional or Student'] = train_set['Working Professional or Student'].map({'Working Professional' : 1, 'Student' : 0}) # Always trasnforming 2 option object types into binary\n",
        "test_set['Working Professional or Student'] = test_set['Working Professional or Student'].map({'Working Professional' : 1, 'Student' : 0})\n",
        "\n",
        "train_set['CGPA'] = train_set['CGPA'] /10\n",
        "train_set['CGPA'] = train_set['CGPA'].fillna(0)\n",
        "test_set['CGPA'] = test_set['CGPA'] /10\n",
        "test_set['CGPA'] = test_set['CGPA'].fillna(0)\n",
        "\n",
        "train_set['Have you ever had suicidal thoughts ?'] = train_set['Have you ever had suicidal thoughts ?'].map({'Yes' : 1, 'No' : 0}) # Always trasnforming 2 option object types into binary\n",
        "test_set['Have you ever had suicidal thoughts ?'] = test_set['Have you ever had suicidal thoughts ?'].map({'Yes' : 1, 'No' : 0})\n",
        "\n",
        "train_set['Work/Study Hours'] = train_set['Work/Study Hours'] / 24 # Normalized by the Max value\n",
        "test_set['Work/Study Hours'] = test_set['Work/Study Hours'] / 24\n",
        "\n",
        "train_set['Financial Stress'] = train_set['Financial Stress'] / 5 # Normalized by the Max value\n",
        "train_set['Financial Stress'] = train_set['Financial Stress'].fillna(0)\n",
        "test_set['Financial Stress'] = test_set['Financial Stress'] / 5\n",
        "test_set['Financial Stress'] = test_set['Financial Stress'].fillna(0)\n",
        "\n",
        "train_set['Family History of Mental Illness'] = train_set['Family History of Mental Illness'].map({'Yes' : 1, 'No' : 0}) # Always trasnforming 2 option object types into binary\n",
        "test_set['Family History of Mental Illness'] = test_set['Family History of Mental Illness'].map({'Yes' : 1, 'No' : 0})"
      ],
      "metadata": {
        "id": "9DqcZW0JLXRz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new feature 'Academic/Work Pressure'\n",
        "train_set['Academic/Work Pressure'] = train_set.apply(\n",
        "    lambda row: row['Academic Pressure'] if pd.notna(row['Academic Pressure']) and pd.isna(row['Work Pressure']) else\n",
        "                row['Work Pressure'] if pd.isna(row['Academic Pressure']) and pd.notna(row['Work Pressure']) else\n",
        "                (row['Academic Pressure'] + row['Work Pressure']) / 2 if pd.notna(row['Academic Pressure']) and pd.notna(row['Work Pressure']) else\n",
        "                None,  # Placeholder for rows where both are NaN\n",
        "    axis=1)\n",
        "\n",
        "# Fill any remaining NaN in 'Academic/Work Pressure' with its mean\n",
        "train_set['Academic/Work Pressure'] = train_set['Academic/Work Pressure'].fillna(train_set['Academic/Work Pressure'].mean())\n",
        "\n",
        "train_set['Academic Pressure'] = train_set['Academic Pressure'] / 5\n",
        "train_set['Academic Pressure'] = train_set['Academic Pressure'].fillna(0)\n",
        "\n",
        "train_set['Work Pressure'] = train_set['Work Pressure'] / 5\n",
        "train_set['Work Pressure'] = train_set['Work Pressure'].fillna(0)"
      ],
      "metadata": {
        "id": "nsrLzYkZNrf8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new feature 'Academic/Work Pressure'\n",
        "test_set['Academic/Work Pressure'] = test_set.apply(\n",
        "    lambda row: row['Academic Pressure'] if pd.notna(row['Academic Pressure']) and pd.isna(row['Work Pressure']) else\n",
        "                row['Work Pressure'] if pd.isna(row['Academic Pressure']) and pd.notna(row['Work Pressure']) else\n",
        "                (row['Academic Pressure'] + row['Work Pressure']) / 2 if pd.notna(row['Academic Pressure']) and pd.notna(row['Work Pressure']) else\n",
        "                None,  # Placeholder for rows where both are NaN\n",
        "    axis=1)\n",
        "\n",
        "# Fill any remaining NaN in 'Academic/Work Pressure' with its mean\n",
        "test_set['Academic/Work Pressure'] = test_set['Academic/Work Pressure'].fillna(test_set['Academic/Work Pressure'].mean())\n",
        "\n",
        "test_set['Academic Pressure'] = test_set['Academic Pressure'] / 5\n",
        "test_set['Academic Pressure'] = test_set['Academic Pressure'].fillna(0)\n",
        "\n",
        "test_set['Work Pressure'] = test_set['Work Pressure'] / 5\n",
        "test_set['Work Pressure'] = test_set['Work Pressure'].fillna(0)"
      ],
      "metadata": {
        "id": "NlJKv80WnYZ3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new feature Study/Job Satisfaction\n",
        "train_set['Study/Job Satisfaction'] = train_set.apply(\n",
        "    lambda row: row['Study Satisfaction'] if pd.notna(row['Study Satisfaction']) and pd.isna(row['Job Satisfaction']) else\n",
        "                row['Job Satisfaction'] if pd.isna(row['Study Satisfaction']) and pd.notna(row['Job Satisfaction']) else\n",
        "                (row['Study Satisfaction'] + row['Job Satisfaction']) / 2 if pd.notna(row['Study Satisfaction']) and pd.notna(row['Job Satisfaction']) else\n",
        "                None,  # Placeholder for rows where both are NaN\n",
        "    axis=1)\n",
        "\n",
        "# Fill any remaining NaN in Study/Job Satisfaction with its mean\n",
        "train_set['Study/Job Satisfaction'] = train_set['Study/Job Satisfaction'].fillna(train_set['Study/Job Satisfaction'].mean())\n",
        "\n",
        "train_set['Study Satisfaction'] = train_set['Study Satisfaction'] / 5\n",
        "train_set['Study Satisfaction'] = train_set['Study Satisfaction'].fillna(0)\n",
        "\n",
        "train_set['Job Satisfaction'] = train_set['Job Satisfaction'] / 5\n",
        "train_set['Job Satisfaction'] = train_set['Job Satisfaction'].fillna(0)"
      ],
      "metadata": {
        "id": "RLBK-FG1Ny4C"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new feature Study/Job Satisfaction\n",
        "test_set['Study/Job Satisfaction'] = test_set.apply(\n",
        "    lambda row: row['Study Satisfaction'] if pd.notna(row['Study Satisfaction']) and pd.isna(row['Job Satisfaction']) else\n",
        "                row['Job Satisfaction'] if pd.isna(row['Study Satisfaction']) and pd.notna(row['Job Satisfaction']) else\n",
        "                (row['Study Satisfaction'] + row['Job Satisfaction']) / 2 if pd.notna(row['Study Satisfaction']) and pd.notna(row['Job Satisfaction']) else\n",
        "                None,  # Placeholder for rows where both are NaN\n",
        "    axis=1)\n",
        "\n",
        "# Fill any remaining NaN in Study/Job Satisfaction with its mean\n",
        "test_set['Study/Job Satisfaction'] = test_set['Study/Job Satisfaction'].fillna(test_set['Study/Job Satisfaction'].mean())\n",
        "\n",
        "test_set['Study Satisfaction'] = test_set['Study Satisfaction'] / 5\n",
        "test_set['Study Satisfaction'] = test_set['Study Satisfaction'].fillna(0)\n",
        "\n",
        "test_set['Job Satisfaction'] = test_set['Job Satisfaction'] / 5\n",
        "test_set['Job Satisfaction'] = test_set['Job Satisfaction'].fillna(0)"
      ],
      "metadata": {
        "id": "8cEjqk0dnhQd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine train and test value counts\n",
        "train_counts = train_set['City'].value_counts()\n",
        "test_counts = test_set['City'].value_counts()\n",
        "\n",
        "# Create a combined count of professions\n",
        "combined_counts = train_counts.add(test_counts, fill_value=0)\n",
        "\n",
        "# Identify professions appearing less than 5 times in either set\n",
        "low_frequency_values = combined_counts[combined_counts < 10].index.tolist()\n",
        "\n",
        "# Map low-frequency values to 'None' in both train and test sets\n",
        "train_set['City'] = train_set['City'].apply(lambda x: 'Other' if x in low_frequency_values else x)\n",
        "test_set['City'] = test_set['City'].apply(lambda x: 'Other' if x in low_frequency_values else x)"
      ],
      "metadata": {
        "id": "WBJlJvkDQNkT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine train and test value counts\n",
        "train_counts = train_set['Profession'].value_counts()\n",
        "test_counts = test_set['Profession'].value_counts()\n",
        "\n",
        "# Create a combined count of professions\n",
        "combined_counts = train_counts.add(test_counts, fill_value=0)\n",
        "\n",
        "# Identify professions appearing less than 5 times in either set\n",
        "low_frequency_values = combined_counts[combined_counts < 10].index.tolist()\n",
        "\n",
        "# Map low-frequency values to 'None' in both train and test sets\n",
        "train_set['Profession'] = train_set['Profession'].apply(lambda x: 'Other' if x in low_frequency_values else x)\n",
        "test_set['Profession'] = test_set['Profession'].apply(lambda x: 'Other' if x in low_frequency_values else x)"
      ],
      "metadata": {
        "id": "jtPh01G6P8IL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine train and test value counts\n",
        "train_counts = train_set['Degree'].value_counts()\n",
        "test_counts = test_set['Degree'].value_counts()\n",
        "\n",
        "# Create a combined count of professions\n",
        "combined_counts = train_counts.add(test_counts, fill_value=0)\n",
        "\n",
        "# Identify professions appearing less than 5 times in either set\n",
        "low_frequency_values = combined_counts[combined_counts < 10].index.tolist()\n",
        "\n",
        "# Map low-frequency values to 'None' in both train and test sets\n",
        "train_set['Degree'] = train_set['Degree'].apply(lambda x: 'Other' if x in low_frequency_values else x)\n",
        "test_set['Degree'] = test_set['Degree'].apply(lambda x: 'Other' if x in low_frequency_values else x)"
      ],
      "metadata": {
        "id": "SXwCvPvIQdpq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set['City'] = train_set['City'].fillna('Other')\n",
        "freq = train_set['City'].value_counts()\n",
        "train_set['City_freq'] = train_set['City'].map(freq) # Frequency encoding different cities\n",
        "\n",
        "test_set['City'] = test_set['City'].fillna('Other')\n",
        "test_set['City_freq'] = test_set['City'].map(freq) # Frequency encoding different cities\n",
        "\n",
        "\n",
        "train_set['Degree'] = train_set['Degree'].fillna('Other')\n",
        "freq = train_set['Degree'].value_counts()\n",
        "train_set['Degree_freq'] = train_set['Degree'].map(freq) # Frequency encoding\n",
        "\n",
        "test_set['Degree'] = test_set['Degree'].fillna('Other')\n",
        "test_set['Degree_freq'] = test_set['Degree'].map(freq) # Frequency encoding\n",
        "\n",
        "\n",
        "train_set['Profession'] = train_set['Profession'].fillna('Other')\n",
        "freq = train_set['Profession'].value_counts()\n",
        "train_set['Profession_freq'] = train_set['Profession'].map(freq) # Frequency encoding\n",
        "\n",
        "test_set['Profession'] = test_set['Profession'].fillna('Other')\n",
        "test_set['Profession_freq'] = test_set['Profession'].map(freq) # Frequency encoding"
      ],
      "metadata": {
        "id": "bm168rqvSxtd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the original number of rows for splitting later\n",
        "train_size = len(train_set)\n",
        "\n",
        "# Concatenate train and test sets\n",
        "concat_set = pd.concat([train_set, test_set], axis=0)\n",
        "\n",
        "# Apply one-hot encoding to the concatenated set\n",
        "concat_set = pd.get_dummies(concat_set, columns=['Degree', 'Profession', 'City'])\n",
        "\n",
        "# Split the concatenated set back into train and test sets\n",
        "train_set = concat_set.iloc[:train_size, :].copy()  # Get the original train rows\n",
        "test_set = concat_set.iloc[train_size:, :].copy()   # Get the original test rows\n",
        "\n",
        "print(\"Train set shape:\", train_set.shape)\n",
        "print(\"Test set shape:\", test_set.shape)"
      ],
      "metadata": {
        "id": "mdoyvMXlhcZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef32479-02e1-487b-8aff-15198e1f405f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (140700, 116)\n",
            "Test set shape: (93800, 116)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Define a function to encode Sleep Duration\n",
        "def encode_sleep_duration(value):\n",
        "    # Check for NaN or None\n",
        "    if pd.isna(value):\n",
        "        return -1  # Placeholder for missing or invalid values\n",
        "\n",
        "    # Ensure the value is a string\n",
        "    value = str(value).strip()\n",
        "\n",
        "    # Encoding logic\n",
        "    if 'Less than 5 hours' in value or value in ['3-4 hours', '4-5 hours', '2-3 hours']:\n",
        "        return 1  # < 5 hours\n",
        "    elif '5-6 hours' in value:\n",
        "        return 2  # 5-6 hours\n",
        "    elif '6-7 hours' in value or '6-8 hours' in value:\n",
        "        return 3  # 6-7 hours\n",
        "    elif '7-8 hours' in value:\n",
        "        return 4  # 7-8 hours\n",
        "    elif 'More than 8 hours' in value or '8-9 hours' in value or '9-11 hours' in value:\n",
        "        return 5  # > 8 hours\n",
        "    else:\n",
        "        return -1  # Placeholder for other invalid or unclear values\n",
        "\n",
        "sleep_mapping = {\n",
        "    'More than 8 hours': '>8',\n",
        "    'Less than 5 hours': '<5',\n",
        "    'Moderate': '5-6 hours',\n",
        "    '9-6 hours': '6-9 hours',\n",
        "\n",
        "    '10-6 hours': '6-10 hours',\n",
        "    'than 5 hours': '4-6 hours',\n",
        "    'Unhealthy': '4-6 hours',\n",
        "    '9-5 hours': '5-9 hours',\n",
        "    '9-5': '5-9 hours',\n",
        "    '8-89 hours': '8-9 hours',\n",
        "\n",
        "    # Ambiguous or irrelevant entries\n",
        "    'Sleep_Duration': None, '40-45 hours': None, '55-66 hours': None, 'Indore': None,\n",
        "    '45': None, '35-36 hours': None, 'No': None, 'Indore': None, '49 hours': None,\n",
        "    'Work_Study_Hours': None, '45-48 hours': None, 'Pune': None, 'Soham': None,\n",
        "    '0': None, 'Meerut': None, '60-65 hours': None, 'Vivan': None,\n",
        "    'Have_you_ever_had_suicidal_thoughts': None, '20-21 hours': None, '50-75 hours': None\n",
        "}\n",
        "\n",
        "# Define range extraction function\n",
        "def extract_range(value):\n",
        "    try:\n",
        "        if '-' in value:\n",
        "            parts = value.split('-')\n",
        "            return int(parts[0]), int(parts[1].split()[0])\n",
        "        elif '<' in value:\n",
        "            return 3, 5\n",
        "        elif '>' in value:\n",
        "            lower_bound = int(value.split('>')[1].split()[0])\n",
        "            return lower_bound, 10\n",
        "        elif value.isdigit():\n",
        "            num = int(value)\n",
        "            return num, num\n",
        "        else:\n",
        "            return None, None\n",
        "    except:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "Kv_Tsp9Gxkl9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set['Sleep Duration'] = train_set['Sleep Duration'].map(sleep_mapping)\n",
        "test_set['Sleep Duration'] = test_set['Sleep Duration'].map(sleep_mapping)\n",
        "\n",
        "train_set['Sleep_Duration_Encoded'] = train_set['Sleep Duration'].apply(encode_sleep_duration)\n",
        "test_set['Sleep_Duration_Encoded'] = test_set['Sleep Duration'].apply(encode_sleep_duration)\n",
        "\n",
        "# Apply the function to create 'From' and 'To' columns\n",
        "train_set[['From', 'To']] = train_set['Sleep Duration'].apply(lambda x: pd.Series(extract_range(x)))\n",
        "test_set[['From', 'To']] = test_set['Sleep Duration'].apply(lambda x: pd.Series(extract_range(x)))\n",
        "\n",
        "# Handle missing values\n",
        "train_set['From'] = train_set['From'].fillna(train_set['From'].mean())\n",
        "train_set['To'] = train_set['To'].fillna(train_set['To'].mean())\n",
        "\n",
        "test_set['From'] = test_set['From'].fillna(test_set['From'].mean())\n",
        "test_set['To'] = test_set['To'].fillna(test_set['To'].mean())\n",
        "\n",
        "# Drop the original column if not needed\n",
        "train_set.drop(columns=['Sleep Duration'], inplace=True)\n",
        "test_set.drop(columns=['Sleep Duration'], inplace=True)"
      ],
      "metadata": {
        "id": "l9dczY77kt9q"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train set shape:\", train_set.shape)\n",
        "print(\"Test set shape:\", test_set.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDylv8whlYyB",
        "outputId": "31046738-43aa-4786-d408-928dfd636cdb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (140700, 118)\n",
            "Test set shape: (93800, 118)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define mapping for valid dietary habits\n",
        "dietary_mapping = {\n",
        "    'Moderate': 'Moderate',\n",
        "    'Unhealthy': 'Unhealthy',\n",
        "    'Healthy': 'Healthy',\n",
        "    'More Healthy': 'Healthy',\n",
        "    'Less Healthy': 'Unhealthy',\n",
        "    'Less than Healthy': 'Unhealthy',\n",
        "    'No Healthy': 'Unhealthy',\n",
        "    '3': 'Moderate',\n",
        "    '1.0': 'Unhealthy',\n",
        "    '2': 'Moderate',\n",
        "    '5 Healthy': 'Healthy',\n",
        "    '5 Unhealthy': 'Unhealthy',\n",
        "\n",
        "    # Ambiguous or irrelevant entries\n",
        "    'Yes': None, 'No': None, 'No Healthy': None, 'Class 12': None, 'Indoor': None,\n",
        "    'Male': None, 'Vegas': None, 'M.Tech': None, 'Electrician': None,\n",
        "    'Hormonal': None, 'Mihir': None, 'Gender': None, 'BSc': None,\n",
        "    'Pratham': None, 'Prachi': None, 'Resistant': None, 'Mealy': None,\n",
        "\n",
        "    'nan': None, 'Academic': None, 'Educational': None, 'Soham': None,\n",
        "    'Naina': None, 'Kolkata': None, 'Raghav': None, 'Vivaan': None,  'MCA': None,\n",
        "}\n",
        "\n",
        "# Apply the mapping to the Dietary Habits column\n",
        "train_set['Dietary Habits'] = train_set['Dietary Habits'].map(dietary_mapping)\n",
        "train_set['Dietary Habits'] = train_set['Dietary Habits'].fillna('X')\n",
        "\n",
        "test_set['Dietary Habits'] = test_set['Dietary Habits'].map(dietary_mapping)\n",
        "test_set['Dietary Habits'] = test_set['Dietary Habits'].fillna('X')\n",
        "\n",
        "freq = train_set['Dietary Habits'].value_counts()\n",
        "train_set['Dietary Habits_freq'] = train_set['Dietary Habits'].map(freq) # frequency encoding\n",
        "test_set['Dietary Habits_freq'] = test_set['Dietary Habits'].map(freq) # frequency encoding\n",
        "\n",
        "# Save the original number of rows for splitting later\n",
        "train_size = len(train_set)\n",
        "\n",
        "# Concatenate train and test sets\n",
        "concat_set = pd.concat([train_set, test_set], axis=0)\n",
        "\n",
        "# Apply one-hot encoding to the concatenated set\n",
        "concat_set = pd.get_dummies(concat_set, columns=['Dietary Habits'])\n",
        "\n",
        "# Split the concatenated set back into train and test sets\n",
        "train_set = concat_set.iloc[:train_size, :].copy()  # Get the original train rows\n",
        "test_set = concat_set.iloc[train_size:, :].copy()   # Get the original test rows"
      ],
      "metadata": {
        "id": "IB0XfR0by2P6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train set shape:\", train_set.shape)\n",
        "print(\"Test set shape:\", test_set.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIvbQJxZh7Rt",
        "outputId": "e363e567-d47c-49da-d038-923180f16aab"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (140700, 122)\n",
            "Test set shape: (93800, 122)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Last Check before tuning"
      ],
      "metadata": {
        "id": "-otnVCUlPdVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for columns with object data type in train_set and test_set\n",
        "train_object_columns = train_set.select_dtypes(include=['object']).columns\n",
        "test_object_columns = test_set.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(\"Object columns in train_set:\", train_object_columns.tolist())\n",
        "print(\"Object columns in test_set:\", test_object_columns.tolist())\n",
        "\n",
        "# Check for columns with missing values in train_set and test_set\n",
        "train_missing = train_set.isna().sum()\n",
        "test_missing = test_set.isna().sum()\n",
        "\n",
        "print(\"\\nColumns with missing values in train_set:\")\n",
        "print(train_missing[train_missing > 0])\n",
        "\n",
        "print(\"\\nColumns with missing values in test_set:\")\n",
        "print(test_missing[test_missing > 0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9bJQt9-mr6C",
        "outputId": "9506eaa8-d85e-4d9d-c85b-33b87295afe5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object columns in train_set: []\n",
            "Object columns in test_set: []\n",
            "\n",
            "Columns with missing values in train_set:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "Columns with missing values in test_set:\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a validation set"
      ],
      "metadata": {
        "id": "syziP-LePQWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_set.copy()\n",
        "y = target_variable\n",
        "\n",
        "# Print the count of True and False in both train and validation sets\n",
        "print(\"Training set class distribution:\\n\", y.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzZnoXOIGbOg",
        "outputId": "1024674b-78c5-4c02-b645-1e2940a19fa3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set class distribution:\n",
            " Depression\n",
            "0    115133\n",
            "1     25567\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter search space\n",
        "search_spaces = {\n",
        "    'learning_rate': Real(0.001, 0.5, 'uniform'),\n",
        "    'max_depth': Integer(0, 150),\n",
        "    'n_estimators': Integer(100, 800),\n",
        "    'subsample': Real(0.5, 1.0, 'uniform'),\n",
        "    'colsample_bytree': Real(0.5, 1.0, 'uniform'),\n",
        "    'gamma': Real(0, 10, 'uniform'),\n",
        "    'reg_alpha': Real(0, 10, 'uniform'),\n",
        "    'reg_lambda': Real(0, 10, 'uniform')\n",
        "}\n",
        "\n",
        "# Initialize the XGBClassifier model with GPU support\n",
        "model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',  # Use for binary classification\n",
        "    tree_method='hist',  # Use GPU for training\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "kf = KFold(n_splits=5)  # 5-fold cross-validation\n",
        "\n",
        "# Use 'accuracy' as the scoring metric\n",
        "optimizer = BayesSearchCV(\n",
        "    estimator=model,\n",
        "    search_spaces=search_spaces,\n",
        "    n_iter=128,\n",
        "    cv=kf,\n",
        "    scoring='accuracy',  # Change scoring metric to accuracy\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the optimizer using X_train and y_train\n",
        "optimizer.fit(X, y)\n",
        "\n",
        "# Get cross-validation results\n",
        "cv_results = optimizer.cv_results_\n",
        "\n",
        "# Create a DataFrame to sort and filter the best models\n",
        "results_df = pd.DataFrame(cv_results)\n",
        "top_15_results = results_df.nlargest(15, 'mean_test_score')  # Get top 5 models by mean test score\n",
        "\n",
        "# Train each of the top 5 models on the full training data and save them\n",
        "top_15_models = []\n",
        "for i, row in top_15_results.iterrows():\n",
        "    params = row['params']\n",
        "    model = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        tree_method='hist',\n",
        "        **params\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    top_15_models.append(model)\n",
        "\n",
        "# Print parameters and scores for the top 5 models\n",
        "for idx, model in enumerate(top_15_models):\n",
        "    print(f\"Model {idx + 1} parameters: {top_15_results.iloc[idx]['params']}\")\n",
        "    print(f\"Model {idx + 1} CV score: {top_15_results.iloc[idx]['mean_test_score']}\")\n",
        "\n",
        "print(\"Top 5 XGBoost models based on accuracy have been trained and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVvwGfYG4nh",
        "outputId": "3a8010d3-aea6-49d4-faa7-7e27f16ddfe4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Model 1 parameters: OrderedDict([('colsample_bytree', 0.7405088127093231), ('gamma', 6.316477657410392), ('learning_rate', 0.10515283828848251), ('max_depth', 41), ('n_estimators', 522), ('reg_alpha', 3.528557641153937), ('reg_lambda', 7.177448862244626), ('subsample', 0.8591451689131435)])\n",
            "Model 1 CV score: 0.9393176972281451\n",
            "Model 2 parameters: OrderedDict([('colsample_bytree', 0.6671555260903674), ('gamma', 3.057058858376773), ('learning_rate', 0.030770907408408214), ('max_depth', 100), ('n_estimators', 541), ('reg_alpha', 9.31680937830822), ('reg_lambda', 3.587438126323634), ('subsample', 0.8021977181801105)])\n",
            "Model 2 CV score: 0.9392821606254442\n",
            "Model 3 parameters: OrderedDict([('colsample_bytree', 0.5721415602288196), ('gamma', 2.393360305054538), ('learning_rate', 0.18163042090227807), ('max_depth', 150), ('n_estimators', 550), ('reg_alpha', 7.688194694944439), ('reg_lambda', 8.657026917782733), ('subsample', 0.6408634902489062)])\n",
            "Model 3 CV score: 0.9392537313432836\n",
            "Model 4 parameters: OrderedDict([('colsample_bytree', 0.5058388483606754), ('gamma', 0.8756270771440234), ('learning_rate', 0.07102245043294252), ('max_depth', 89), ('n_estimators', 174), ('reg_alpha', 9.027099176573767), ('reg_lambda', 4.94031698283343), ('subsample', 0.6244897574007454)])\n",
            "Model 4 CV score: 0.939225302061123\n",
            "Model 5 parameters: OrderedDict([('colsample_bytree', 0.873618581515144), ('gamma', 4.416769656869429), ('learning_rate', 0.02015968472500549), ('max_depth', 105), ('n_estimators', 642), ('reg_alpha', 0.17729299380851773), ('reg_lambda', 6.505470434688857), ('subsample', 0.9142957219619905)])\n",
            "Model 5 CV score: 0.9391613361762616\n",
            "Model 6 parameters: OrderedDict([('colsample_bytree', 0.5138106776913312), ('gamma', 1.951669307708241), ('learning_rate', 0.27280665446028934), ('max_depth', 144), ('n_estimators', 778), ('reg_alpha', 8.731563507497242), ('reg_lambda', 2.9129508163894196), ('subsample', 0.9074108909036831)])\n",
            "Model 6 CV score: 0.9391613361762614\n",
            "Model 7 parameters: OrderedDict([('colsample_bytree', 1.0), ('gamma', 10.0), ('learning_rate', 0.067827459746308), ('max_depth', 0), ('n_estimators', 800), ('reg_alpha', 0.0), ('reg_lambda', 1.9535528782379212), ('subsample', 0.7178697997732547)])\n",
            "Model 7 CV score: 0.9391115849324805\n",
            "Model 8 parameters: OrderedDict([('colsample_bytree', 0.5069518662798271), ('gamma', 5.446662628449061), ('learning_rate', 0.2187049983072964), ('max_depth', 0), ('n_estimators', 626), ('reg_alpha', 7.752191818481471), ('reg_lambda', 4.087070804226648), ('subsample', 0.5)])\n",
            "Model 8 CV score: 0.9390902629708601\n",
            "Model 9 parameters: OrderedDict([('colsample_bytree', 0.6801816252009699), ('gamma', 6.389609931158945), ('learning_rate', 0.06865208949462707), ('max_depth', 108), ('n_estimators', 154), ('reg_alpha', 1.430277893511466), ('reg_lambda', 1.5305579097270574), ('subsample', 0.5022124018348499)])\n",
            "Model 9 CV score: 0.9390902629708601\n",
            "Model 10 parameters: OrderedDict([('colsample_bytree', 0.5), ('gamma', 10.0), ('learning_rate', 0.11218331972180147), ('max_depth', 0), ('n_estimators', 385), ('reg_alpha', 0.0), ('reg_lambda', 0.0), ('subsample', 0.9613594394759126)])\n",
            "Model 10 CV score: 0.9390831556503197\n",
            "Model 11 parameters: OrderedDict([('colsample_bytree', 0.6314215020526701), ('gamma', 7.657665369013198), ('learning_rate', 0.1578007327590151), ('max_depth', 58), ('n_estimators', 535), ('reg_alpha', 2.1283566951449835), ('reg_lambda', 1.7179542752267665), ('subsample', 0.7944829487585383)])\n",
            "Model 11 CV score: 0.9390618336886993\n",
            "Model 12 parameters: OrderedDict([('colsample_bytree', 0.512506700373853), ('gamma', 10.0), ('learning_rate', 0.08848588607722285), ('max_depth', 88), ('n_estimators', 800), ('reg_alpha', 0.0), ('reg_lambda', 10.0), ('subsample', 1.0)])\n",
            "Model 12 CV score: 0.9390618336886993\n",
            "Model 13 parameters: OrderedDict([('colsample_bytree', 1.0), ('gamma', 10.0), ('learning_rate', 0.052539832397324036), ('max_depth', 150), ('n_estimators', 800), ('reg_alpha', 0.0), ('reg_lambda', 10.0), ('subsample', 0.7900811284288545)])\n",
            "Model 13 CV score: 0.9390476190476191\n",
            "Model 14 parameters: OrderedDict([('colsample_bytree', 0.5403743889264079), ('gamma', 9.316415738345187), ('learning_rate', 0.12328675845396979), ('max_depth', 7), ('n_estimators', 509), ('reg_alpha', 1.1465702562265292), ('reg_lambda', 8.599383204676933), ('subsample', 0.5697098233375238)])\n",
            "Model 14 CV score: 0.9390262970859986\n",
            "Model 15 parameters: OrderedDict([('colsample_bytree', 0.5), ('gamma', 10.0), ('learning_rate', 0.2551983942829529), ('max_depth', 0), ('n_estimators', 725), ('reg_alpha', 1.394064912745384), ('reg_lambda', 5.247146236814148), ('subsample', 0.5)])\n",
            "Model 15 CV score: 0.9390191897654585\n",
            "Top 5 XGBoost models based on accuracy have been trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provided model parameters\n",
        "model_parameters = [\n",
        "    {'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.17498071644573762, 'max_depth': 2,\n",
        "     'n_estimators': 500, 'reg_alpha': 0.0, 'reg_lambda': 9.151145867495522, 'subsample': 0.5},\n",
        "    {'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.2549581698910242, 'max_depth': 2,\n",
        "     'n_estimators': 265, 'reg_alpha': 4.038289521295645, 'reg_lambda': 10.0, 'subsample': 0.9960867858635486},\n",
        "    {'colsample_bytree': 0.8795506374579873, 'gamma': 0.0, 'learning_rate': 0.30421049078309637, 'max_depth': 2,\n",
        "     'n_estimators': 486, 'reg_alpha': 0.0, 'reg_lambda': 8.866224462355815, 'subsample': 0.9749930308767123},\n",
        "    {'colsample_bytree': 0.5, 'gamma': 0.0, 'learning_rate': 0.16493489467143477, 'max_depth': 2,\n",
        "     'n_estimators': 483, 'reg_alpha': 3.3240779723362874, 'reg_lambda': 0.0, 'subsample': 0.5338992950605351},\n",
        "    {'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.15176332186610494, 'max_depth': 2,\n",
        "     'n_estimators': 500, 'reg_alpha': 6.242954728815679, 'reg_lambda': 10.0, 'subsample': 0.5},\n",
        "    {'colsample_bytree': 0.5, 'gamma': 0.0, 'learning_rate': 0.13040617716396105, 'max_depth': 3,\n",
        "     'n_estimators': 206, 'reg_alpha': 0.0, 'reg_lambda': 0.10848609573480272, 'subsample': 0.5},\n",
        "    {'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.16770145742460443, 'max_depth': 2,\n",
        "     'n_estimators': 458, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 0.5},\n",
        "    {'colsample_bytree': 0.8129503564108058, 'gamma': 0.0, 'learning_rate': 0.15802399297592248, 'max_depth': 2,\n",
        "     'n_estimators': 500, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 0.7829437669435633},\n",
        "    {'colsample_bytree': 0.5472210854922583, 'gamma': 0.36891517213378583, 'learning_rate': 0.2042057552362013,\n",
        "     'max_depth': 3, 'n_estimators': 477, 'reg_alpha': 9.418738411067523, 'reg_lambda': 9.882994460606403, 'subsample': 0.6009453161350771},\n",
        "    {'colsample_bytree': 0.5, 'gamma': 3.8813896117418527, 'learning_rate': 0.23171050419161035, 'max_depth': 2,\n",
        "     'n_estimators': 500, 'reg_alpha': 9.299861755925473, 'reg_lambda': 9.860246011624099, 'subsample': 0.5},\n",
        "    {'colsample_bytree': 0.5187090871871713, 'gamma': 1.9371693716260263, 'learning_rate': 0.08135956297056286,\n",
        "     'max_depth': 53, 'n_estimators': 391, 'reg_alpha': 9.572358158935941, 'reg_lambda': 0.3420957950727914, 'subsample': 0.5990142142660962},\n",
        "    {'colsample_bytree': 0.5375742997971107, 'gamma': 1.4236931028578583, 'learning_rate': 0.17856328866164642,\n",
        "     'max_depth': 3, 'n_estimators': 221, 'reg_alpha': 4.49220653483089, 'reg_lambda': 7.841135497386919, 'subsample': 0.799031526237848},\n",
        "    {'colsample_bytree': 0.6558257910115323, 'gamma': 1.7417863837760763, 'learning_rate': 0.05590885653026205,\n",
        "     'max_depth': 3, 'n_estimators': 435, 'reg_alpha': 9.858087481320169, 'reg_lambda': 0.34221958921542345, 'subsample': 0.9809061058838722},\n",
        "    {'colsample_bytree': 0.5742198876061808, 'gamma': 2.464551380974906, 'learning_rate': 0.09180938836310773,\n",
        "     'max_depth': 2, 'n_estimators': 375, 'reg_alpha': 4.614280204189711, 'reg_lambda': 4.688090936618526, 'subsample': 0.9982576387424718},\n",
        "    {'colsample_bytree': 0.5, 'gamma': 10.0, 'learning_rate': 0.17462497230623591, 'max_depth': 2,\n",
        "     'n_estimators': 500, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 0.5}\n",
        "]\n",
        "\n",
        "# Initialize arrays to store predictions\n",
        "train_predictions = []\n",
        "val_predictions = []\n",
        "test_predictions = []\n",
        "\n",
        "# Train models and collect predictions\n",
        "for idx, params in enumerate(model_parameters, 1):\n",
        "    print(f\"Training Model {idx} with parameters: {params}\")\n",
        "\n",
        "    # Initialize XGBoost model with parameters\n",
        "    model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='error', **params)\n",
        "\n",
        "    # Train the model on the meta-train set\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on train, validation, and test sets\n",
        "    train_pred = model.predict_proba(X_train)[:, 1]\n",
        "    train_acc = accuracy_score(y_train, (train_pred > 0.5).astype(int))\n",
        "\n",
        "    val_pred = model.predict_proba(X_val)[:, 1]\n",
        "    val_acc = accuracy_score(y_val, (val_pred > 0.5).astype(int))\n",
        "\n",
        "    test_pred = model.predict_proba(test_set)[:, 1]\n",
        "    print(f\"Train-Val Accuracy: {train_acc}-{val_acc}\")\n",
        "    # Append predictions to the respective arrays\n",
        "    train_predictions.append(train_pred)\n",
        "    val_predictions.append(val_pred)\n",
        "    test_predictions.append(test_pred)"
      ],
      "metadata": {
        "id": "N77LsygjI6cI",
        "outputId": "876b46d6-a6f2-4cbe-945e-25645e1d7784",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model 1 with parameters: {'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.17498071644573762, 'max_depth': 2, 'n_estimators': 500, 'reg_alpha': 0.0, 'reg_lambda': 9.151145867495522, 'subsample': 0.5}\n",
            "Train-Val Accuracy: 0.9414988549316907-0.9394456289978678\n",
            "Training Model 2 with parameters: {'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.2549581698910242, 'max_depth': 2, 'n_estimators': 265, 'reg_alpha': 4.038289521295645, 'reg_lambda': 10.0, 'subsample': 0.9960867858635486}\n",
            "Train-Val Accuracy: 0.9411197978362158-0.9403695806680882\n",
            "Training Model 3 with parameters: {'colsample_bytree': 0.8795506374579873, 'gamma': 0.0, 'learning_rate': 0.30421049078309637, 'max_depth': 2, 'n_estimators': 486, 'reg_alpha': 0.0, 'reg_lambda': 8.866224462355815, 'subsample': 0.9749930308767123}\n",
            "Train-Val Accuracy: 0.9423438363736871-0.9398009950248756\n",
            "Training Model 4 with parameters: {'colsample_bytree': 0.5, 'gamma': 0.0, 'learning_rate': 0.16493489467143477, 'max_depth': 2, 'n_estimators': 483, 'reg_alpha': 3.3240779723362874, 'reg_lambda': 0.0, 'subsample': 0.5338992950605351}\n",
            "Train-Val Accuracy: 0.9412145621100845-0.9398720682302771\n",
            "Training Model 5 with parameters: {'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.15176332186610494, 'max_depth': 2, 'n_estimators': 500, 'reg_alpha': 6.242954728815679, 'reg_lambda': 10.0, 'subsample': 0.5}\n",
            "Train-Val Accuracy: 0.9411671799731501-0.9398009950248756\n",
            "Training Model 6 with parameters: {'colsample_bytree': 0.5, 'gamma': 0.0, 'learning_rate': 0.13040617716396105, 'max_depth': 3, 'n_estimators': 206, 'reg_alpha': 0.0, 'reg_lambda': 0.10848609573480272, 'subsample': 0.5}\n",
            "Train-Val Accuracy: 0.9417673537076522-0.9408670931058991\n",
            "Training Model 7 with parameters: {'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.16770145742460443, 'max_depth': 2, 'n_estimators': 458, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 0.5}\n",
            "Train-Val Accuracy: 0.9410013424938798-0.9396588486140725\n",
            "Training Model 8 with parameters: {'colsample_bytree': 0.8129503564108058, 'gamma': 0.0, 'learning_rate': 0.15802399297592248, 'max_depth': 2, 'n_estimators': 500, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 0.7829437669435633}\n",
            "Train-Val Accuracy: 0.9417515596620074-0.9400852878464819\n",
            "Training Model 9 with parameters: {'colsample_bytree': 0.5472210854922583, 'gamma': 0.36891517213378583, 'learning_rate': 0.2042057552362013, 'max_depth': 3, 'n_estimators': 477, 'reg_alpha': 9.418738411067523, 'reg_lambda': 9.882994460606403, 'subsample': 0.6009453161350771}\n",
            "Train-Val Accuracy: 0.9427860696517413-0.9398720682302771\n",
            "Training Model 10 with parameters: {'colsample_bytree': 0.5, 'gamma': 3.8813896117418527, 'learning_rate': 0.23171050419161035, 'max_depth': 2, 'n_estimators': 500, 'reg_alpha': 9.299861755925473, 'reg_lambda': 9.860246011624099, 'subsample': 0.5}\n",
            "Train-Val Accuracy: 0.9396588486140725-0.9398009950248756\n",
            "Training Model 11 with parameters: {'colsample_bytree': 0.5187090871871713, 'gamma': 1.9371693716260263, 'learning_rate': 0.08135956297056286, 'max_depth': 53, 'n_estimators': 391, 'reg_alpha': 9.572358158935941, 'reg_lambda': 0.3420957950727914, 'subsample': 0.5990142142660962}\n",
            "Train-Val Accuracy: 0.9437416094132512-0.9396588486140725\n",
            "Training Model 12 with parameters: {'colsample_bytree': 0.5375742997971107, 'gamma': 1.4236931028578583, 'learning_rate': 0.17856328866164642, 'max_depth': 3, 'n_estimators': 221, 'reg_alpha': 4.49220653483089, 'reg_lambda': 7.841135497386919, 'subsample': 0.799031526237848}\n",
            "Train-Val Accuracy: 0.9414277817262892-0.9389481165600568\n",
            "Training Model 13 with parameters: {'colsample_bytree': 0.6558257910115323, 'gamma': 1.7417863837760763, 'learning_rate': 0.05590885653026205, 'max_depth': 3, 'n_estimators': 435, 'reg_alpha': 9.858087481320169, 'reg_lambda': 0.34221958921542345, 'subsample': 0.9809061058838722}\n",
            "Train-Val Accuracy: 0.9405591092158256-0.9399431414356787\n",
            "Training Model 14 with parameters: {'colsample_bytree': 0.5742198876061808, 'gamma': 2.464551380974906, 'learning_rate': 0.09180938836310773, 'max_depth': 2, 'n_estimators': 375, 'reg_alpha': 4.614280204189711, 'reg_lambda': 4.688090936618526, 'subsample': 0.9982576387424718}\n",
            "Train-Val Accuracy: 0.9401405670062386-0.9404406538734897\n",
            "Training Model 15 with parameters: {'colsample_bytree': 0.5, 'gamma': 10.0, 'learning_rate': 0.17462497230623591, 'max_depth': 2, 'n_estimators': 500, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 0.5}\n",
            "Train-Val Accuracy: 0.9398167890705205-0.9395167022032693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert train_predictions (list) to NumPy array\n",
        "train_predictions_array = np.column_stack(train_predictions)\n",
        "\n",
        "# Convert train_predictions_array to a DataFrame\n",
        "train_predictions_df = pd.DataFrame(train_predictions_array, columns=[f\"Model_{i+1}\" for i in range(train_predictions_array.shape[1])])\n",
        "\n",
        "# Similarly for val_predictions and test_predictions\n",
        "val_predictions_array = np.column_stack(val_predictions)\n",
        "val_predictions_df = pd.DataFrame(val_predictions_array, columns=[f\"Model_{i+1}\" for i in range(val_predictions_array.shape[1])])\n",
        "\n",
        "test_predictions_array = np.column_stack(test_predictions)\n",
        "test_predictions_df = pd.DataFrame(test_predictions_array, columns=[f\"Model_{i+1}\" for i in range(test_predictions_array.shape[1])])\n",
        "\n",
        "# Inspect one of the DataFrames\n",
        "print(train_predictions_df.head())"
      ],
      "metadata": {
        "id": "g9WqkZkKLtcn",
        "outputId": "f407755e-ed4d-484f-abbb-e6d7ce960ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Model_1   Model_2   Model_3   Model_4   Model_5   Model_6   Model_7  \\\n",
            "0  0.251242  0.226676  0.199309  0.235344  0.188756  0.174904  0.198623   \n",
            "1  0.011884  0.007121  0.007199  0.009190  0.006693  0.007105  0.010315   \n",
            "2  0.084804  0.102299  0.097618  0.118540  0.133718  0.084927  0.134447   \n",
            "3  0.000767  0.000892  0.000593  0.000648  0.000681  0.001192  0.000824   \n",
            "4  0.978974  0.978859  0.984682  0.982335  0.977579  0.968099  0.980950   \n",
            "\n",
            "    Model_8   Model_9  Model_10  Model_11  Model_12  Model_13  Model_14  \\\n",
            "0  0.201382  0.276731  0.207906  0.237120  0.231641  0.181192  0.218942   \n",
            "1  0.007006  0.006442  0.005668  0.008108  0.007390  0.008129  0.007450   \n",
            "2  0.076002  0.094696  0.081722  0.100928  0.084072  0.108933  0.105172   \n",
            "3  0.000711  0.000431  0.000779  0.001496  0.000958  0.001315  0.001490   \n",
            "4  0.979025  0.986072  0.971625  0.976665  0.973480  0.968814  0.973640   \n",
            "\n",
            "   Model_15  \n",
            "0  0.192652  \n",
            "1  0.007817  \n",
            "2  0.102633  \n",
            "3  0.001524  \n",
            "4  0.969462  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Train the Logistic Regressor as a meta-model\n",
        "meta_model = LogisticRegression(max_iter=10000)\n",
        "meta_model.fit(val_predictions_df, y_val)\n",
        "\n",
        "# Predict on validation set\n",
        "meta_val_pred = meta_model.predict(val_predictions_df)\n",
        "\n",
        "# Evaluate the meta-model\n",
        "print(\"Classification Report - Meta Model (Validation Set):\")\n",
        "print(classification_report(y_val, meta_val_pred))\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, meta_val_pred):.4f}\")\n",
        "\n",
        "# If test_predictions_array is ready\n",
        "meta_test_pred = meta_model.predict(test_predictions_df)\n",
        "\n",
        "from google.colab import files\n",
        "# Prepare the output DataFrame\n",
        "output = pd.DataFrame({'id': test_id.values, 'loan_status': meta_test_pred})\n",
        "\n",
        "# Remove any duplicate rows by 'PassengerId'\n",
        "output.drop_duplicates(subset='id', keep='first', inplace=True)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "output.to_csv('predictions.csv', index=False)\n",
        "files.download('predictions.csv')"
      ],
      "metadata": {
        "id": "RVuAHpaBMnNc",
        "outputId": "5cba77b8-285a-4034-b1ef-1ba141f08408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report - Meta Model (Validation Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96     11513\n",
            "           1       0.86      0.80      0.83      2557\n",
            "\n",
            "    accuracy                           0.94     14070\n",
            "   macro avg       0.91      0.89      0.90     14070\n",
            "weighted avg       0.94      0.94      0.94     14070\n",
            "\n",
            "Validation Accuracy: 0.9402\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2469152a-5f15-4b91-8003-119e3bb4b029\", \"predictions.csv\", 844215)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "params1 = {\n",
        "    'colsample_bytree': 0.7405088127093231,\n",
        "    'gamma': 6.316477657410392,\n",
        "    'learning_rate': 0.10515283828848251,\n",
        "    'max_depth': 41,\n",
        "    'n_estimators': 522,\n",
        "    'reg_alpha': 3.528557641153937,\n",
        "    'reg_lambda': 7.177448862244626,\n",
        "    'subsample': 0.8591451689131435\n",
        "}\n",
        "# Define the model with the provided parameters\n",
        "xgb_model1 = xgb.XGBClassifier(**params1, eval_metric='error')\n",
        "\n",
        "# Train the model\n",
        "xgb_model1.fit(train_set, target_variable)\n",
        "\n",
        "y_pred_prob_XGB_test = xgb_model1.predict(test_set)\n",
        "\n",
        "output = pd.DataFrame({'id': test_id.values, 'loan_status': y_pred_prob_XGB_test})\n",
        "\n",
        "# Remove any duplicate rows by 'PassengerId'\n",
        "output.drop_duplicates(subset='id', keep='first', inplace=True)\n",
        "from google.colab import files\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "output.to_csv('predictions.csv', index=False)\n",
        "files.download('predictions.csv')"
      ],
      "metadata": {
        "id": "i7SYboydOX-R",
        "outputId": "4f758201-3900-4f68-8ac0-4fe5b58daa10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1e1c8c00-d903-4518-950f-e7fff745d84c\", \"predictions.csv\", 844215)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}